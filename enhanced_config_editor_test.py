#!/usr/bin/env python3
"""
Enhanced Configuration Editor Focused Test Suite
Tests the specific functionality requested in the review:
1. Configuration Retrieval (GET endpoints)
2. Configuration Updates (PUT endpoints with dry_run modes)
3. Validation Endpoints
4. Diff Calculation
5. Authentication
6. Error Handling
"""

import requests
import json
import sys
from datetime import datetime

class EnhancedConfigEditorTester:
    def __init__(self, base_url="https://editor-bounds-fix.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.token = None
        self.tests_run = 0
        self.tests_passed = 0
        self.critical_failures = []

    def run_test(self, name, method, endpoint, expected_status, data=None, headers=None, params=None):
        """Run a single API test"""
        url = f"{self.api_url}/{endpoint}"
        test_headers = {'Content-Type': 'application/json'}
        
        if self.token:
            test_headers['Authorization'] = f'Bearer {self.token}'
        
        if headers:
            test_headers.update(headers)

        self.tests_run += 1
        print(f"\nüîç Testing {name}...")
        print(f"   URL: {url}")
        
        try:
            if method == 'GET':
                response = requests.get(url, headers=test_headers, params=params, timeout=10)
            elif method == 'POST':
                response = requests.post(url, json=data, headers=test_headers, params=params, timeout=10)
            elif method == 'PUT':
                response = requests.put(url, json=data, headers=test_headers, params=params, timeout=10)

            success = response.status_code == expected_status
            if success:
                self.tests_passed += 1
                print(f"‚úÖ Passed - Status: {response.status_code}")
                try:
                    response_data = response.json()
                    if isinstance(response_data, dict) and len(str(response_data)) < 800:
                        print(f"   Response: {json.dumps(response_data, indent=2)}")
                    elif isinstance(response_data, list):
                        print(f"   Response: List with {len(response_data)} items")
                    else:
                        print(f"   Response: Large object with keys: {list(response_data.keys()) if isinstance(response_data, dict) else 'Non-dict response'}")
                except:
                    print(f"   Response: {response.text[:200]}...")
            else:
                print(f"‚ùå Failed - Expected {expected_status}, got {response.status_code}")
                print(f"   Response: {response.text[:300]}")
                self.critical_failures.append(name)

            return success, response.json() if response.content else {}

        except requests.exceptions.RequestException as e:
            print(f"‚ùå Failed - Network Error: {str(e)}")
            self.critical_failures.append(name)
            return False, {}
        except Exception as e:
            print(f"‚ùå Failed - Error: {str(e)}")
            self.critical_failures.append(name)
            return False, {}

    def test_authentication(self):
        """Test authentication with admin credentials"""
        print("\n" + "="*80)
        print("üîê TESTING AUTHENTICATION")
        print("="*80)
        
        success, response = self.run_test(
            "Admin Login for Enhanced Config Editor",
            "POST",
            "auth/login",
            200,
            data={"username": "admin", "password": "admin123"}
        )
        
        if success and 'access_token' in response:
            self.token = response['access_token']
            print(f"   ‚úÖ Authenticated as: {response.get('user', {}).get('username')} ({response.get('user', {}).get('role')})")
            return True
        else:
            print("   ‚ùå Authentication failed - cannot proceed with protected endpoints")
            return False

    def test_configuration_retrieval(self):
        """Test GET endpoints for deployments and daemonsets configuration"""
        print("\n" + "="*80)
        print("üì• TESTING CONFIGURATION RETRIEVAL")
        print("="*80)
        
        # First get available resources
        success, deployments = self.run_test(
            "List Available Deployments",
            "GET",
            "deployments",
            200
        )
        
        success, daemonsets = self.run_test(
            "List Available DaemonSets", 
            "GET",
            "daemonsets",
            200
        )
        
        if not deployments or not daemonsets:
            print("‚ùå No resources available for configuration testing")
            return False
        
        # Test deployment configuration retrieval
        deployment = deployments[0]
        dep_namespace = deployment.get('namespace', 'default')
        dep_name = deployment.get('name', 'nginx-deployment')
        
        success1, dep_config = self.run_test(
            f"GET Deployment Config ({dep_namespace}/{dep_name})",
            "GET",
            f"deployment/{dep_namespace}/{dep_name}/config",
            200
        )
        
        # Test daemonset configuration retrieval
        daemonset = daemonsets[0]
        ds_namespace = daemonset.get('namespace', 'kube-system')
        ds_name = daemonset.get('name', 'datadog-agent')
        
        success2, ds_config = self.run_test(
            f"GET DaemonSet Config ({ds_namespace}/{ds_name})",
            "GET",
            f"daemonset/{ds_namespace}/{ds_name}/config",
            200
        )
        
        # Validate configuration structure
        if success1 and dep_config:
            required_fields = ['apiVersion', 'kind', 'metadata', 'spec']
            missing_fields = [field for field in required_fields if field not in dep_config]
            if not missing_fields:
                print(f"   ‚úÖ Deployment config has all required fields: {required_fields}")
            else:
                print(f"   ‚ö†Ô∏è Deployment config missing fields: {missing_fields}")
        
        if success2 and ds_config:
            required_fields = ['apiVersion', 'kind', 'metadata', 'spec']
            missing_fields = [field for field in required_fields if field not in ds_config]
            if not missing_fields:
                print(f"   ‚úÖ DaemonSet config has all required fields: {required_fields}")
            else:
                print(f"   ‚ö†Ô∏è DaemonSet config missing fields: {missing_fields}")
        
        return success1 and success2

    def test_configuration_updates(self):
        """Test PUT endpoints with dry_run=true and dry_run=false modes"""
        print("\n" + "="*80)
        print("üìù TESTING CONFIGURATION UPDATES")
        print("="*80)
        
        # Get deployment for testing
        success, deployments = self.run_test(
            "Get Deployments for Update Testing",
            "GET", 
            "deployments",
            200
        )
        
        if not deployments:
            print("‚ùå No deployments available for update testing")
            return False
        
        deployment = deployments[0]
        namespace = deployment.get('namespace', 'default')
        name = deployment.get('name', 'nginx-deployment')
        
        # Get current configuration
        success, current_config = self.run_test(
            f"Get Current Config for Updates ({namespace}/{name})",
            "GET",
            f"deployment/{namespace}/{name}/config",
            200
        )
        
        if not success or not current_config:
            print("‚ùå Cannot get current configuration for update testing")
            return False
        
        # Test 1: Dry run mode (dry_run=true)
        test_config = current_config.copy()
        if 'spec' not in test_config:
            test_config['spec'] = {}
        test_config['spec']['replicas'] = 4  # Change replicas for testing
        
        success1, dry_run_result = self.run_test(
            f"PUT Config with dry_run=true ({namespace}/{name})",
            "PUT",
            f"deployment/{namespace}/{name}/config",
            200,
            data={
                "configuration": test_config,
                "dry_run": True,
                "strategy": "merge"
            }
        )
        
        if success1:
            if dry_run_result.get('dry_run') == True:
                print(f"   ‚úÖ Dry run mode confirmed - Changes detected: {len(dry_run_result.get('applied_changes', []))}")
            else:
                print(f"   ‚ö†Ô∏è Dry run flag not properly set in response")
        
        # Test 2: Actual application (dry_run=false)
        test_config['spec']['replicas'] = 3  # Different value for actual application
        
        success2, apply_result = self.run_test(
            f"PUT Config with dry_run=false ({namespace}/{name})",
            "PUT",
            f"deployment/{namespace}/{name}/config",
            200,
            data={
                "configuration": test_config,
                "dry_run": False,
                "strategy": "merge"
            }
        )
        
        if success2:
            if apply_result.get('success') == True:
                print(f"   ‚úÖ Configuration applied successfully - Rollback key: {apply_result.get('rollback_key', 'N/A')}")
            else:
                print(f"   ‚ö†Ô∏è Configuration application reported as unsuccessful")
        
        # Test 3: DaemonSet update with dry run
        success, daemonsets = self.run_test(
            "Get DaemonSets for Update Testing",
            "GET",
            "daemonsets", 
            200
        )
        
        if daemonsets:
            daemonset = daemonsets[0]
            ds_namespace = daemonset.get('namespace', 'kube-system')
            ds_name = daemonset.get('name', 'datadog-agent')
            
            # Get current daemonset config
            success, ds_config = self.run_test(
                f"Get DaemonSet Config for Updates ({ds_namespace}/{ds_name})",
                "GET",
                f"daemonset/{ds_namespace}/{ds_name}/config",
                200
            )
            
            if success and ds_config:
                # Modify daemonset config (add a label)
                test_ds_config = ds_config.copy()
                if 'metadata' not in test_ds_config:
                    test_ds_config['metadata'] = {}
                if 'labels' not in test_ds_config['metadata']:
                    test_ds_config['metadata']['labels'] = {}
                test_ds_config['metadata']['labels']['test-update'] = 'enhanced-config-editor'
                
                success3, ds_dry_run = self.run_test(
                    f"PUT DaemonSet Config with dry_run=true ({ds_namespace}/{ds_name})",
                    "PUT",
                    f"daemonset/{ds_namespace}/{ds_name}/config",
                    200,
                    data={
                        "configuration": test_ds_config,
                        "dry_run": True,
                        "strategy": "merge"
                    }
                )
                
                if success3 and ds_dry_run.get('dry_run') == True:
                    print(f"   ‚úÖ DaemonSet dry run successful - Changes: {len(ds_dry_run.get('applied_changes', []))}")
        
        return success1 and success2

    def test_validation_endpoints(self):
        """Test configuration validation endpoint"""
        print("\n" + "="*80)
        print("‚úÖ TESTING VALIDATION ENDPOINTS")
        print("="*80)
        
        # Test 1: Valid deployment configuration
        valid_deployment_config = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": "test-deployment",
                "namespace": "default",
                "labels": {"app": "test-app", "version": "v1.0"}
            },
            "spec": {
                "replicas": 3,
                "selector": {
                    "matchLabels": {"app": "test-app"}
                },
                "template": {
                    "metadata": {
                        "labels": {"app": "test-app"}
                    },
                    "spec": {
                        "containers": [{
                            "name": "test-container",
                            "image": "nginx:1.21",
                            "ports": [{"containerPort": 80}],
                            "resources": {
                                "requests": {"memory": "64Mi", "cpu": "250m"},
                                "limits": {"memory": "128Mi", "cpu": "500m"}
                            }
                        }]
                    }
                }
            }
        }
        
        success1, validation_result = self.run_test(
            "Validate Valid Deployment Configuration",
            "POST",
            "validate-config",
            200,
            data=valid_deployment_config,
            params={"resource_type": "deployment"}
        )
        
        if success1:
            if validation_result.get('valid') == True:
                print(f"   ‚úÖ Valid configuration correctly validated")
            else:
                print(f"   ‚ö†Ô∏è Valid configuration marked as invalid: {validation_result.get('validation_errors', [])}")
        
        # Test 2: Invalid deployment configuration (missing required fields)
        invalid_deployment_config = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": "invalid-deployment",
                "namespace": "default"
            },
            "spec": {
                "replicas": -1,  # Invalid negative replicas
                # Missing selector and template
            }
        }
        
        success2, invalid_result = self.run_test(
            "Validate Invalid Deployment Configuration",
            "POST",
            "validate-config",
            200,
            data=invalid_deployment_config,
            params={"resource_type": "deployment"}
        )
        
        if success2:
            if validation_result.get('valid') == False or len(invalid_result.get('validation_errors', [])) > 0:
                print(f"   ‚úÖ Invalid configuration correctly rejected with {len(invalid_result.get('validation_errors', []))} errors")
            else:
                print(f"   ‚ö†Ô∏è Invalid configuration incorrectly accepted")
        
        # Test 3: Valid DaemonSet configuration
        valid_daemonset_config = {
            "apiVersion": "apps/v1",
            "kind": "DaemonSet",
            "metadata": {
                "name": "test-daemonset",
                "namespace": "kube-system",
                "labels": {"app": "test-daemon", "component": "monitoring"}
            },
            "spec": {
                "selector": {
                    "matchLabels": {"app": "test-daemon"}
                },
                "template": {
                    "metadata": {
                        "labels": {"app": "test-daemon"}
                    },
                    "spec": {
                        "containers": [{
                            "name": "daemon-container",
                            "image": "busybox:latest",
                            "command": ["sleep", "3600"]
                        }]
                    }
                }
            }
        }
        
        success3, ds_validation = self.run_test(
            "Validate Valid DaemonSet Configuration",
            "POST",
            "validate-config",
            200,
            data=valid_daemonset_config,
            params={"resource_type": "daemonset"}
        )
        
        if success3 and ds_validation.get('valid') == True:
            print(f"   ‚úÖ Valid DaemonSet configuration correctly validated")
        
        return success1 and success2 and success3

    def test_diff_calculation(self):
        """Test configuration diff endpoint"""
        print("\n" + "="*80)
        print("üîç TESTING DIFF CALCULATION")
        print("="*80)
        
        # Test 1: Simple configuration diff
        original_config = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": "test-app",
                "namespace": "default",
                "labels": {"app": "test", "version": "v1.0"}
            },
            "spec": {
                "replicas": 2,
                "template": {
                    "spec": {
                        "containers": [{
                            "name": "app",
                            "image": "nginx:1.20",
                            "ports": [{"containerPort": 80}]
                        }]
                    }
                }
            }
        }
        
        updated_config = {
            "apiVersion": "apps/v1",
            "kind": "Deployment",
            "metadata": {
                "name": "test-app",
                "namespace": "default",
                "labels": {"app": "test", "version": "v2.0"}  # Changed version
            },
            "spec": {
                "replicas": 5,  # Changed replicas
                "template": {
                    "spec": {
                        "containers": [{
                            "name": "app",
                            "image": "nginx:1.21",  # Changed image
                            "ports": [{"containerPort": 80}],
                            "env": [{"name": "ENV", "value": "production"}]  # Added env
                        }]
                    }
                }
            }
        }
        
        success1, diff_result = self.run_test(
            "Calculate Configuration Diff - Multiple Changes",
            "POST",
            "config-diff",
            200,
            data={
                "original_config": original_config,
                "updated_config": updated_config
            }
        )
        
        if success1:
            if diff_result.get('has_changes') == True:
                print(f"   ‚úÖ Configuration differences correctly detected")
                diff_data = diff_result.get('diff', {})
                if 'values_changed' in diff_data:
                    changes_count = len(diff_data['values_changed'])
                    print(f"   üìä Detected {changes_count} field changes")
                else:
                    print(f"   üìä Diff structure: {list(diff_data.keys())}")
            else:
                print(f"   ‚ö†Ô∏è Configuration differences not detected")
        
        # Test 2: No changes diff
        success2, no_diff_result = self.run_test(
            "Calculate Configuration Diff - No Changes",
            "POST",
            "config-diff",
            200,
            data={
                "original_config": original_config,
                "updated_config": original_config  # Same config
            }
        )
        
        if success2:
            if not no_diff_result.get('has_changes', True):
                print(f"   ‚úÖ No changes correctly detected")
            else:
                print(f"   ‚ö†Ô∏è False positive - changes detected when none exist")
        
        # Test 3: Complex nested changes
        complex_original = {
            "spec": {
                "template": {
                    "spec": {
                        "containers": [{
                            "name": "app",
                            "env": [
                                {"name": "DB_HOST", "value": "localhost"},
                                {"name": "DB_PORT", "value": "5432"}
                            ]
                        }]
                    }
                }
            }
        }
        
        complex_updated = {
            "spec": {
                "template": {
                    "spec": {
                        "containers": [{
                            "name": "app",
                            "env": [
                                {"name": "DB_HOST", "value": "postgres.example.com"},  # Changed
                                {"name": "DB_PORT", "value": "5432"},
                                {"name": "DB_NAME", "value": "production"}  # Added
                            ]
                        }]
                    }
                }
            }
        }
        
        success3, complex_diff = self.run_test(
            "Calculate Configuration Diff - Complex Nested Changes",
            "POST",
            "config-diff",
            200,
            data={
                "original_config": complex_original,
                "updated_config": complex_updated
            }
        )
        
        if success3 and complex_diff.get('has_changes') == True:
            print(f"   ‚úÖ Complex nested changes correctly detected")
        
        return success1 and success2 and success3

    def test_error_handling(self):
        """Test error handling with invalid configurations and missing resources"""
        print("\n" + "="*80)
        print("üö® TESTING ERROR HANDLING")
        print("="*80)
        
        # Test 1: Invalid resource type
        success1, error_result = self.run_test(
            "GET Config - Invalid Resource Type",
            "GET",
            "invalidtype/default/test/config",
            400  # Should return 400 Bad Request
        )
        
        # Test 2: Missing resource
        success2, missing_result = self.run_test(
            "GET Config - Non-existent Resource",
            "GET",
            "deployment/default/non-existent-deployment/config",
            404  # Should return 404 Not Found
        )
        
        # Test 3: Invalid namespace format
        success3, invalid_ns = self.run_test(
            "GET Config - Invalid Namespace Format",
            "GET",
            "deployment/INVALID-NAMESPACE/test/config",
            400  # Should return 400 Bad Request
        )
        
        # Test 4: Malformed configuration update
        malformed_config = {
            "configuration": "this-is-not-a-dict",  # Invalid configuration format
            "dry_run": True
        }
        
        success4, malformed_result = self.run_test(
            "PUT Config - Malformed Configuration",
            "PUT",
            "deployment/default/nginx-deployment/config",
            422  # Should return 422 Unprocessable Entity or 400
        )
        
        # Test 5: Missing required fields in validation
        success5, missing_fields = self.run_test(
            "Validate Config - Missing Required Fields",
            "POST",
            "validate-config",
            400,  # Should return 400 for missing resource_type
            data={"invalid": "config"}
        )
        
        # Test 6: Invalid diff request
        success6, invalid_diff = self.run_test(
            "Config Diff - Missing Required Fields",
            "POST",
            "config-diff",
            400,  # Should return 400 for missing configs
            data={"only_original": {"spec": {"replicas": 1}}}
        )
        
        error_tests_passed = sum([success1, success2, success3, success5, success6])
        print(f"\n   üìä Error handling tests: {error_tests_passed}/5 passed (malformed config test may vary)")
        
        return error_tests_passed >= 4  # Allow some flexibility for malformed config test

    def test_authentication_requirements(self):
        """Test that all endpoints properly require authentication"""
        print("\n" + "="*80)
        print("üîê TESTING AUTHENTICATION REQUIREMENTS")
        print("="*80)
        
        # Save current token
        original_token = self.token
        self.token = None  # Remove authentication
        
        # Test endpoints without authentication
        endpoints_to_test = [
            ("GET Deployment Config - No Auth", "GET", "deployment/default/nginx-deployment/config", 401),
            ("PUT Deployment Config - No Auth", "PUT", "deployment/default/nginx-deployment/config", 401),
            ("Validate Config - No Auth", "POST", "validate-config", 401),
            ("Config Diff - No Auth", "POST", "config-diff", 401),
            ("List Deployments - No Auth", "GET", "deployments", 401),
            ("List DaemonSets - No Auth", "GET", "daemonsets", 401)
        ]
        
        auth_tests_passed = 0
        for test_name, method, endpoint, expected_status in endpoints_to_test:
            success, _ = self.run_test(test_name, method, endpoint, expected_status)
            if success:
                auth_tests_passed += 1
        
        # Test with invalid token
        self.token = "invalid_token_12345"
        success, _ = self.run_test(
            "GET Config - Invalid Token",
            "GET",
            "deployment/default/nginx-deployment/config",
            401
        )
        if success:
            auth_tests_passed += 1
        
        # Restore original token
        self.token = original_token
        
        print(f"\n   üìä Authentication requirement tests: {auth_tests_passed}/{len(endpoints_to_test) + 1} passed")
        return auth_tests_passed >= len(endpoints_to_test)

    def run_comprehensive_test(self):
        """Run all Enhanced Configuration Editor tests"""
        print("üöÄ Enhanced Configuration Editor Comprehensive Test Suite")
        print("=" * 80)
        print("Testing Enhanced Configuration Editor functionality:")
        print("1. Configuration Retrieval (GET endpoints)")
        print("2. Configuration Updates (PUT endpoints with dry_run modes)")
        print("3. Validation Endpoints")
        print("4. Diff Calculation")
        print("5. Authentication Requirements")
        print("6. Error Handling")
        print("=" * 80)
        
        # Run test suite
        test_results = {}
        
        # Authentication (prerequisite)
        test_results['authentication'] = self.test_authentication()
        if not test_results['authentication']:
            print("\n‚ùå CRITICAL: Authentication failed - cannot proceed with protected endpoint tests")
            return False
        
        # Core functionality tests
        test_results['configuration_retrieval'] = self.test_configuration_retrieval()
        test_results['configuration_updates'] = self.test_configuration_updates()
        test_results['validation_endpoints'] = self.test_validation_endpoints()
        test_results['diff_calculation'] = self.test_diff_calculation()
        test_results['error_handling'] = self.test_error_handling()
        test_results['authentication_requirements'] = self.test_authentication_requirements()
        
        # Summary
        print("\n" + "=" * 80)
        print("üìä ENHANCED CONFIGURATION EDITOR TEST RESULTS")
        print("=" * 80)
        
        passed_tests = sum(test_results.values())
        total_tests = len(test_results)
        
        for test_name, result in test_results.items():
            status = "‚úÖ PASSED" if result else "‚ùå FAILED"
            print(f"{status} - {test_name.replace('_', ' ').title()}")
        
        print(f"\nüìà Overall Results: {self.tests_passed}/{self.tests_run} individual tests passed")
        print(f"üìà Test Categories: {passed_tests}/{total_tests} categories passed")
        
        if self.critical_failures:
            print(f"\nüö® Critical Failures ({len(self.critical_failures)}):")
            for failure in self.critical_failures:
                print(f"   ‚ùå {failure}")
        
        # Final assessment
        if passed_tests == total_tests and len(self.critical_failures) == 0:
            print("\nüéâ ALL ENHANCED CONFIGURATION EDITOR TESTS PASSED!")
            print("‚úÖ The Enhanced Configuration Editor is fully functional and ready for production use.")
            return True
        elif passed_tests >= total_tests * 0.8:
            print("\n‚úÖ MOST ENHANCED CONFIGURATION EDITOR TESTS PASSED!")
            print("‚ö†Ô∏è Some minor issues detected but core functionality is working.")
            return True
        else:
            print("\n‚ùå ENHANCED CONFIGURATION EDITOR HAS SIGNIFICANT ISSUES!")
            print("üîß Critical functionality is not working properly.")
            return False

def main():
    tester = EnhancedConfigEditorTester()
    success = tester.run_comprehensive_test()
    return 0 if success else 1

if __name__ == "__main__":
    sys.exit(main())